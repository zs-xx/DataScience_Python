{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936f6d5e-11d0-4ba5-9724-06d723c3c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist \n",
    "# 28x28 pixels (matrix), grayscale 0-255\n",
    "# 784 pixels - each pixel is input for NN\n",
    "# 2 hidden layers\n",
    "# 10 digits - 10 classes - 10 output units \n",
    "# one hot encoding\n",
    "# softmax activation func\n",
    "# mnist data is iterable and 2-tuple format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de86af0-ec4b-4e6f-b205-cd713a69b0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 18:25:02.487664: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-09-16 18:25:02.487688: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-09-16 18:25:02.487694: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-09-16 18:25:02.487846: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-16 18:25:02.487859: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e7818b-2783-4e5b-a957-a1c465df2a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 18:25:03.067187: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "/opt/anaconda3/envs/tf/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-09-16 18:25:03.336286: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 - 8s - 11ms/step - accuracy: 0.8982 - loss: 0.3524 - val_accuracy: 0.9452 - val_loss: 0.1846\n",
      "Epoch 2/5\n",
      "720/720 - 7s - 9ms/step - accuracy: 0.9504 - loss: 0.1659 - val_accuracy: 0.9598 - val_loss: 0.1370\n",
      "Epoch 3/5\n",
      "720/720 - 7s - 9ms/step - accuracy: 0.9604 - loss: 0.1297 - val_accuracy: 0.9675 - val_loss: 0.1172\n",
      "Epoch 4/5\n",
      "720/720 - 6s - 9ms/step - accuracy: 0.9665 - loss: 0.1085 - val_accuracy: 0.9675 - val_loss: 0.1083\n",
      "Epoch 5/5\n",
      "720/720 - 6s - 9ms/step - accuracy: 0.9706 - loss: 0.0941 - val_accuracy: 0.9700 - val_loss: 0.0991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15e469cd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255. # between 0 and 1\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# shuffle \n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# batch size 1 sgd\n",
    "# batch size # samples single batch gd\n",
    "# when batching we find avg loss\n",
    "BATCH_SIZE = 75\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples) # single batch\n",
    "test_data = test_data.batch(num_test_samples) # single batch\n",
    "\n",
    "# makes validation_data iterator\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 75\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # transforms tensor into a vector\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f83370-7b3e-46a4-8415-7ac5605dfcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9648 - loss: 0.1181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.11811891943216324, 0.9648000001907349)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "test_loss, test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
